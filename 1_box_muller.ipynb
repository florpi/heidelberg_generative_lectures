{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39246e6b",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/florpi/heidelberg_generative_lectures/blob/main/1_box_muller.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b126b4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy matplotlib scipy torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e83c7ff",
   "metadata": {},
   "source": [
    "\n",
    "# The Box-Muller Transform\n",
    "\n",
    "## Why This Matters for Generative Models\n",
    "\n",
    "In generative modeling, we often need to transform simple distributions (like uniform or standard normal) into complex ones that represent our data. The Box-Muller transform, developed by George Box and Mervin Muller in 1934, is one of the earliest and most elegant examples of such transformations. More importantly, it serves as a perfect introduction to the mathematical principles underlying **normalizing flows** - a powerful class of generative models that learn to transform simple distributions into complex ones.\n",
    "\n",
    "By the end of this tutorial, you will understand:\n",
    "\n",
    "1. **The fundamental problem**: How to generate samples from more complex distributions using simple ones\n",
    "2. **The mathematical foundation**: The change of variables rule and its role in probability transformations\n",
    "3. **Practical implementation**: How to implement such transformations using automatic differentiation methods\n",
    "\n",
    "## The Box-Muller Transform\n",
    "\n",
    "The Box-Muller transform converts two independent uniform random variables into two independent standard normal random variables. Given $U_1, U_2 \\sim \\text{Uniform}(0,1)$, the transformation:\n",
    "\n",
    "$$Z_0 = \\sqrt{-2 \\ln U_1} \\cos(2\\pi U_2)$$\n",
    "$$Z_1 = \\sqrt{-2 \\ln U_1} \\sin(2\\pi U_2)$$\n",
    "\n",
    "produces $Z_0, Z_1 \\sim \\mathcal{N}(0,1)$.\n",
    "\n",
    "In esence it is an invertible, differentiable transformations that can reshape probability distributions while maintaining tractable densities.\n",
    "\n",
    "Let's explore this step by step!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975fb44b",
   "metadata": {},
   "source": [
    "## Implementation: \n",
    "\n",
    "Let's implement the Box-Muller transform and verify that it works as expected. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a041f215",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm, multivariate_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67835126",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def box_muller_transform(u1, u2):\n",
    "    \"\"\"\n",
    "    Box-Muller transform: (u1, u2) -> (z1, z2)\n",
    "    where u1, u2 ~ Uniform(0,1) and z1, z2 ~ Normal(0,1)\n",
    "    \"\"\"\n",
    "    #TODO: Implement the Box-Muller transform\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def318e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from 2d uniform distribution\n",
    "\n",
    "u1 = np.random.uniform(0, 1, 2_000)\n",
    "u2 = np.random.uniform(0, 1, 2_000)\n",
    "\n",
    "# transform to normal distribution\n",
    "z1, z2 = box_muller_transform(u1, u2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3934d78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, figsize=(10,4))\n",
    "_ = ax[0].hist(u1, bins=30, density=True, alpha=0.3,)\n",
    "_ = ax[0].hist(u2, bins=30, density=True, alpha=0.3,)\n",
    "_ = ax[1].hist(z1, bins=30, density=True, alpha=0.3,)\n",
    "_ = ax[1].hist(z2, bins=30, density=True, alpha=0.3,)\n",
    "\n",
    "x_plot = np.linspace(-3, 3, 100)\n",
    "\n",
    "ax[1].plot(\n",
    "    x_plot,\n",
    "    norm.pdf(x_plot),\n",
    "    label=r\"$\\mathcal{N}(0, 1)$\",\n",
    "    c='k',\n",
    ")\n",
    "ax[1].legend()\n",
    "\n",
    "ax[0].set_title(r'$U_1, U_2 \\sim U$')\n",
    "ax[1].set_title(r'$Z_1, Z_2 \\sim \\mathcal{N}(0,1)$')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8335cc75",
   "metadata": {},
   "source": [
    "## The Change of Variables Rule \n",
    "\n",
    "The beauty of the Box-Muller transform lies not just in its simplicity, but in how it demonstrates the **change of variables rule** for byjective functions - the foundation of normalizing flows.\n",
    "\n",
    "### The Change of Variables Rule \n",
    "\n",
    "When we transform random variables using a differentiable, **invertible** function $f: u \\rightarrow z$, the probability density changes according to:\n",
    "\n",
    "$$p_Z(z) = p_U(u=f^{-1}(z)) \\left| \\frac{\\partial f^{-1}(z)}{\\partial z} \\right|$$\n",
    "\n",
    "Where:\n",
    "- $p_Z(z)$ is the density of the transformed variable\n",
    "- $p_U(u)$ is the density of the original variable  \n",
    "- $f^{-1}$ is the inverse transformation\n",
    "- $\\left| \\frac{\\partial f^{-1}(z)}{\\partial z} \\right|$ is the absolute value of the Jacobian determinant\n",
    "\n",
    "### Why This Matters for Normalizing Flows\n",
    "\n",
    "In normalizing flows, we:\n",
    "1. **Learn** the transformation $f_\\theta$ (instead of using a fixed one like Box-Muller)\n",
    "2. **Compute** the Jacobian determinant efficiently using automatic differentiation\n",
    "3. **Use** this to compute exact likelihoods for training\n",
    "\n",
    "Let's verify that Box-Muller satisfies this theorem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f489f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute the Jacobian of the transformation\n",
    "def analytical_jacobian_determinant(z1, z2):\n",
    "    return\n",
    "\n",
    "\n",
    "# TODO: Compute the transformed PDF\n",
    "def compute_transformed_pdf(z1, z2, u1, u2):\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "pdf_jacobian = compute_transformed_pdf(z1, z2, u1, u2)\n",
    "pdf_analytical = multivariate_normal.pdf(np.column_stack((z1, z2)), mean=[0, 0], cov=[[1, 0], [0, 1]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3756598f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a777d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pdf_analytical, pdf_jacobian, marker='o', alpha=0.6, markersize=1,)\n",
    "plt.xlabel('True PDF')\n",
    "plt.ylabel('PDF')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5e1e58",
   "metadata": {},
   "source": [
    "## Automatic Differentiation\n",
    "\n",
    "As you've learned, manually computing Jacobians becomes impractical for complex transformations. This is where **automatic differentiation** becomes crucial.\n",
    "\n",
    "### Why Automatic Differentiation?\n",
    "\n",
    "1. **Scalability**: Hand-deriving Jacobians for complex neural networks is impossible\n",
    "2. **Efficiency**: Autodiff computes exact derivatives in the same time as forward pass\n",
    "3. **Flexibility**: We can easily change architectures without re-deriving derivatives\n",
    "4. **Numerical Stability**: Avoids approximation errors from numerical differentiation\n",
    "\n",
    "\n",
    "Let's implement the autodiff equivalent using PyTorch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d63c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Device setup - automatically detect and use GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# TODO: Write the inverse Box-Muller transform and use autodiff to compute the Jacobia\n",
    "def inverse_box_muller(z):\n",
    "    \"\"\"\n",
    "    Inverse Box-Muller: (z1, z2) -> (u1, u2)\n",
    "    \"\"\"\n",
    "    return\n",
    "\n",
    "\n",
    "def compute_jacobian_autograd(func, inputs):\n",
    "\n",
    "    \"\"\"\n",
    "    Compute Jacobian using PyTorch's autograd.\n",
    "    For a function f: R^n -> R^m, computes the m x n Jacobian matrix.\n",
    "    \"\"\"\n",
    "    return\n",
    "\n",
    "\n",
    "# Convert NumPy arrays to PyTorch tensors on the correct device\n",
    "z = torch.tensor(np.column_stack((z1, z2)), dtype=torch.float32, device=device)\n",
    "\n",
    "autodiff_jacobian = compute_jacobian_autograd(inverse_box_muller, z).detach()\n",
    "pdf_autodiff = 1.0 * torch.abs(torch.det(autodiff_jacobian))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5669de43",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pdf_analytical, pdf_autodiff, marker='o', alpha=0.6, markersize=1, linestyle='')\n",
    "plt.xlabel('True PDF')\n",
    "plt.ylabel('PDF')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e022bf64",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
