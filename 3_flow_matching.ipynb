{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d8f16f1",
   "metadata": {},
   "source": [
    "In this tutorial, we'll implement flow matching from scratch. Remember that the difference with CNFs will be a simulation-free loss that does not require solving an ode during training, by constraining the sampling trajectory.\n",
    "\n",
    "Let's break down the key equations that we will use:\n",
    "\n",
    "1. **The Flow Equation**:\n",
    "    As in CNFs:\n",
    "   $\\frac{dx_t}{dt} = v_\\theta(x_t, t)$\n",
    "\n",
    "   Where:\n",
    "   - $x_t$ is our image as it's being transformed\n",
    "   - $t$ is time (between 0 and 1), at t=0 $p(x_0)$ is $N(0,1)$, at t=1, $p(x_1)$ is the target distribution we wish to sample from.\n",
    "   - $v_\\theta$ is a velocity field we'll parametrise with a neural network, and that will determine the direction in which we need to move to sample from the target distribution. \n",
    "\n",
    "2. **The Path** (Linear interpolation from noise to data)\n",
    "\n",
    "   We chose an interpolant, $x_t$, that obeys:\n",
    "\n",
    "   $x_t = (1-t)x_0 + tx_1$\n",
    "\n",
    "   Where:\n",
    "   - $x_0$ is random noise (our starting point)\n",
    "   - $x_1$ is a real galaxy image (our target) \n",
    "\n",
    "3. **Loss Function**:\n",
    "   We train our model to minimize:\n",
    "\n",
    "   $L = \\mathbb{E}_{t,x_0,x_1} \\left[ \\left\\| v_\\theta(x_t, t) - u(x_0,x_1,t) \\right\\|^2 \\right]$\n",
    "\n",
    "   Where, given the linear interpolant, the true velocity field is $u(x_0, x_1,t) = x_1 - x_0$. This teaches our model to predict the right direction of change at each step.\n",
    "\n",
    "\n",
    "Once trained, we can use the learned velocity field to generate new galaxy-like images by:\n",
    "1. Starting with random noise\n",
    "2. Solving the ODE to transform the noise into a sample of the data distribution. \n",
    "\n",
    "\n",
    "Now, let's go back to our favourite example, the two moons :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea3b285",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision \n",
    "%pip install matplotlib\n",
    "%pip install numpy\n",
    "%pip install tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8684fc26",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/florpi/heidelberg_generative_lectures/blob/main/3_flow_matching.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767f725a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6c3bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_moons(n: int, sigma: float = 5e-2):\n",
    "    theta = 2 * torch.pi * torch.rand(n)\n",
    "    label = (theta > torch.pi).float()\n",
    "\n",
    "    x = torch.stack(\n",
    "        (\n",
    "            torch.cos(theta) + label - 1 / 2,\n",
    "            torch.sin(theta) + label / 2 - 1 / 4,\n",
    "        ),\n",
    "        axis=-1,\n",
    "    )\n",
    "\n",
    "    return torch.normal(x, sigma), label\n",
    "\n",
    "\n",
    "samples, labels = two_moons(16384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac28180",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(4.8, 4.8))\n",
    "plt.hist2d(*samples.T, bins=64, range=((-2, 2), (-2, 2)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc3278a",
   "metadata": {},
   "source": [
    "Now let's implement the interpolant to generate $x_t$ and check it visually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8427ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolant(x0, x1, t):\n",
    "    \"\"\"\n",
    "    #TODO:\n",
    "\n",
    "    Implement the linear interpolation between noise (x0) and data (x1).\n",
    "    \n",
    "    At t=0: we should get pure noise (x0)\n",
    "    At t=1: we should get real data (x1) \n",
    "    At t=0.5: we should get a 50/50 mix\n",
    "    \n",
    "    Args:\n",
    "        x0: Starting point (noise), shape: (batch_size, ...)\n",
    "        x1: End point (data), shape: (batch_size, ...)\n",
    "        t: Time parameter, shape: (batch_size,) or scalar\n",
    "        \n",
    "    Returns:\n",
    "        x_t: Interpolated samples at time t\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    return \n",
    "\n",
    "fig, axs = plt.subplots(1, 10, figsize=(20, 2))\n",
    "\n",
    "x1, _ = two_moons(500,)\n",
    "x1 = torch.tensor(x1).float()\n",
    "x0  = torch.randn_like(x1)\n",
    "t = torch.tensor(np.linspace(0., 1., 10))\n",
    "for i, ax in enumerate(axs):\n",
    "    xt = interpolant(x0,x1,t[i])\n",
    "    ax.scatter(xt[:,0], xt[:,1],s=1, color='k')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f't={t[i]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee259ce4",
   "metadata": {},
   "source": [
    "As in the previous tutorial the velocity model will take $x_t$ and t as inputs, and predict the velocity\n",
    "\n",
    "Let's write a neural network that can do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d84cbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class VelocityNetwork(nn.Module):\n",
    "    def __init__(self, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        # TODO: Create a neural network that takes as input:\n",
    "        # - 2D position (x_t): 2 dimensions  \n",
    "        # - 1D time (t): 1 dimension\n",
    "        # Total input: 3 dimensions\n",
    "        # \n",
    "        # Output: 2D velocity vector\n",
    "        #\n",
    "        # Store this in self.net as a nn.Sequential\n",
    "        \n",
    "        #self.net = # Your code here\n",
    "\n",
    "    def forward(self, xt, t):\n",
    "        # TODO: Combine xt and t into a single input tensor and pass it through the \n",
    "        # velocity prediction network\n",
    "        return\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0301de82",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "velocity_nn = VelocityNetwork(\n",
    "    hidden_dim=256,\n",
    ").to(device)\n",
    "t=torch.tensor(500*[0.5],dtype=torch.float32, device=device)\n",
    "\n",
    "assert velocity_nn(\n",
    "    xt=x1.to(device),\n",
    "    t=t,\n",
    ").shape == (500, 2), \"Expected shape (500, 2) but got different shape\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae20106",
   "metadata": {},
   "source": [
    "**Now let's write down the flow matching model**:\n",
    "\n",
    "Computing the loss is simple:\n",
    "\n",
    "1. **Sample a random training pair**: Pick noise $x_0$ and real data $x_1$\n",
    "2. **Pick a random time**: Choose $t$ uniformly between 0 and 1 to Monte Carlo estimate the expectation value over time.\n",
    "3. **Get the interpolated data**: $x_t = (1-t)x_0 + tx_1$\n",
    "4. **Evaluate the velocity prediction network**: \"What velocity do you predict at $(x_t, t)$?\"\n",
    "5. **Compare with truth when knowing $x_1$**\n",
    "6. **Update weights**: Use backpropagation to improve predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff09bfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowMatching(nn.Module):\n",
    "    def __init__(self, velocity_network, device='cuda'):\n",
    "        super().__init__()\n",
    "        self.velocity_net = velocity_network\n",
    "        self.device = device\n",
    "        \n",
    "    def sample_time(self, batch_size):\n",
    "        \"\"\"Sample random time steps between 0 and 1.\"\"\"\n",
    "        return torch.rand(batch_size, device=self.device)\n",
    "    \n",
    "    def interpolate(self, x0, x1, t):\n",
    "        \"\"\"\n",
    "        Linear interpolation between noise (x0) and data (x1).\n",
    "        At t=0: return x0 (noise)\n",
    "        At t=1: return x1 (data)\n",
    "        \"\"\"\n",
    "        # TODO: fill in the interpolant again\n",
    "        return\n",
    "    \n",
    "    def compute_target_velocity(self, x0, x1, t):\n",
    "        \"\"\"\n",
    "        #TODO\n",
    "        Compute the target velocity field that the neural network should learn.\n",
    "        \n",
    "        Think about this: if we have x_t = (1-t)*x0 + t*x1, \n",
    "        what is dx_t/dt (the derivative with respect to time)?\n",
    "        \"\"\"\n",
    "        # Your code here\n",
    "        #return # What is dx_t/dt?\n",
    "    \n",
    "    def compute_loss(self, x1,):\n",
    "        \"\"\"\n",
    "        Compute flow matching loss.\n",
    "        \n",
    "        Args:\n",
    "            x1: data samples\n",
    "        \n",
    "        Returns:\n",
    "            Loss value\n",
    "        \"\"\"\n",
    "        # TODO: compute the flow matching loss given the functions above\n",
    "        return\n",
    "\n",
    "    \n",
    "    def sample(self, target_shape=(2,), num_samples=10, num_steps=200,):\n",
    "        \"\"\"\n",
    "        Generate samples by integrating the learned velocity field.\n",
    "        \n",
    "        Args:\n",
    "            num_samples: Number of samples to generate\n",
    "            num_steps: Number of integration steps\n",
    "        \n",
    "        Returns:\n",
    "            Generated samples\n",
    "        \"\"\"\n",
    "        self.velocity_net.eval()\n",
    "        # TODO: Solve the ODE via Euler integration\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43302733",
   "metadata": {},
   "outputs": [],
   "source": [
    "fm = FlowMatching(\n",
    "    velocity_network=velocity_nn,\n",
    "    device=device,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440afe20",
   "metadata": {},
   "source": [
    "First, let's check that we can call compute_loss. If our data is standarized, we should expect a randomly initialized velocity network to return something of the order of 1. Always pay attention to the initial values of the loss funciton, as huge or tiny values usually indicate bugs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896bfd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fm.compute_loss(\n",
    "    x1=x1.to(device),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281f5bcd",
   "metadata": {},
   "source": [
    "Now let's check the ODE solver code to generate new samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ac0559",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = fm.sample(\n",
    "   num_samples=500,\n",
    "\n",
    ").cpu().numpy()\n",
    "\n",
    "assert samples.shape == (500, 2), \"Expected shape (500, 2) but got different shape\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b369f6",
   "metadata": {},
   "source": [
    "If we plot them, they should look completely random given that the velocity network hasn't been trained yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8013f7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    samples[:,0],\n",
    "    samples[:,1],\n",
    "    c='k',\n",
    "    s=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43557eba",
   "metadata": {},
   "source": [
    "Now let's write the training loop to improve our currently very useless velocity model :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2e13a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from IPython.display import clear_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93af7bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(fm.parameters(), lr=1.e-4, weight_decay=1e-4)\n",
    "total_steps = 40_000\n",
    "generate_every = 100\n",
    "batch_size = 128\n",
    "losses = []\n",
    "\n",
    "fm.train()\n",
    "\n",
    "step = 0 \n",
    "with tqdm(total=total_steps, desc=\"Training\") as pbar:\n",
    "    while step < total_steps:\n",
    "        x, label = two_moons(batch_size)\n",
    "        x = x.to(device)  # Move data to device\n",
    "        loss = fm.compute_loss(x).mean()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(fm.parameters(), max_norm=1.0)\n",
    "\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "        pbar.set_postfix({\n",
    "                'Loss': f'{loss:.4f}',\n",
    "            })\n",
    "        pbar.update(1)\n",
    "        step += 1\n",
    "        losses.append(loss.detach())\n",
    "        if step % generate_every == 0:\n",
    "            print(f\"\\nGenerating samples at step {step}...\")\n",
    "            \n",
    "            samples = fm.sample(\n",
    "                num_samples=500, \n",
    "            ).detach().cpu().numpy()\n",
    "            clear_output(wait=True)\n",
    "            plt.scatter(\n",
    "                x[:,0].cpu().numpy(),\n",
    "                x[:,1].cpu().numpy(),\n",
    "                c='indianred',\n",
    "                s=1,\n",
    "                alpha=0.8,\n",
    "                label='Original'\n",
    "            )\n",
    "            plt.scatter(\n",
    "                samples[:,0],\n",
    "                samples[:,1],\n",
    "                c='k',\n",
    "                s=1,\n",
    "                label='Generated'\n",
    "            )\n",
    "            plt.legend()\n",
    "            plt.title(f'Generated Samples at step {step}')\n",
    "            plt.axis('off')\n",
    " \n",
    "            plt.show()\n",
    "            fm.velocity_net.train()\n",
    "\n",
    "    losses = torch.stack(losses)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227200ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = fm.sample(\n",
    "    num_samples=500, \n",
    ").detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fe6408",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(4.8, 4.8))\n",
    "plt.hist2d(*samples.T, bins=64, range=((-2, 2), (-2, 2)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fed07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edfc62e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
