{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56bce08e",
   "metadata": {},
   "source": [
    "This tutorial takes a hands-on, open-ended approachâ€”think of it as your chance to get more comfortable with PyTorch by building your own flow matching code from scratch. We provide boilerplate code for data loading and point you to useful velocity models in existing PyTorch libraries, but the implementation details are yours to explore!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40fb1d3",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/florpi/heidelberg_generative_lectures/blob/main/4_galaxy_images.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d791e4",
   "metadata": {},
   "source": [
    "Let's first write our data loading code. We will use galaxy image data from AstroClip (https://arxiv.org/abs/2310.03024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f0d713",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Optional\n",
    "from lightning import LightningDataModule\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torchvision.transforms import CenterCrop\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ToRGB:\n",
    "    \"\"\"\n",
    "    Transformation from raw image data (nanomaggies) to the rgb values displayed\n",
    "    at the legacy viewer https://www.legacysurvey.org/viewer\n",
    "\n",
    "    Code copied from\n",
    "    https://github.com/legacysurvey/imagine/blob/master/map/views.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scales=None, m=0.03, Q=20, bands=[\"g\", \"r\", \"z\"]):\n",
    "        rgb_scales = {\n",
    "            \"u\": (2, 1.5),\n",
    "            \"g\": (2, 6.0),\n",
    "            \"r\": (1, 3.4),\n",
    "            \"i\": (0, 1.0),\n",
    "            \"z\": (0, 2.2),\n",
    "        }\n",
    "        if scales is not None:\n",
    "            rgb_scales.update(scales)\n",
    "\n",
    "        self.rgb_scales = rgb_scales\n",
    "        self.m = m\n",
    "        self.Q = Q\n",
    "        self.bands = bands\n",
    "        self.axes, self.scales = zip(*[rgb_scales[bands[i]] for i in range(len(bands))])\n",
    "\n",
    "        # rearange scales to correspond to image channels after swapping\n",
    "        self.scales = [self.scales[i] for i in self.axes]\n",
    "\n",
    "    def __call__(self, imgs):\n",
    "        # Check image shape and set to C x H x W\n",
    "        if imgs.shape[0] != len(self.bands):\n",
    "            imgs = np.transpose(imgs, (2, 0, 1))\n",
    "\n",
    "        I = 0\n",
    "        for img, band in zip(imgs, self.bands):\n",
    "            plane, scale = self.rgb_scales[band]\n",
    "            img = np.maximum(0, img * scale + self.m)\n",
    "            I = I + img\n",
    "        I /= len(self.bands)\n",
    "\n",
    "        Q = 20\n",
    "        fI = np.arcsinh(Q * I) / np.sqrt(Q)\n",
    "        I += (I == 0.0) * 1e-6\n",
    "        H, W = I.shape\n",
    "        rgb = np.zeros((H, W, 3), np.float32)\n",
    "        for img, band in zip(imgs, self.bands):\n",
    "            plane, scale = self.rgb_scales[band]\n",
    "            rgb[:, :, plane] = (img * scale + self.m) * fI / I\n",
    "\n",
    "        rgb = np.clip(rgb, 0, 1)\n",
    "        return rgb\n",
    "\n",
    "\n",
    "class AstroClipCollator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        center_crop: int = 144,\n",
    "        bands: List[str] = [\"g\", \"r\", \"z\"],\n",
    "        m: float = 0.03,\n",
    "        Q: int = 20,\n",
    "    ):\n",
    "        self.center_crop = CenterCrop(center_crop)\n",
    "        self.to_rgb = ToRGB(bands=bands, m=m, Q=Q)\n",
    "\n",
    "    def _process_images(self, images):\n",
    "        # convert to rgb\n",
    "        img_outs = []\n",
    "        for img in images:\n",
    "            rgb_img = torch.tensor(self.to_rgb(img)[None, :, :, :])\n",
    "            img_outs.append(rgb_img)\n",
    "        images = torch.concatenate(img_outs)\n",
    "\n",
    "        images = self.center_crop(images.permute(0, 3, 2, 1))\n",
    "        return images\n",
    "\n",
    "    def __call__(self, samples):\n",
    "        # collate and handle dimensions\n",
    "        samples = default_collate(samples)\n",
    "        # process images\n",
    "        samples[\"image\"] = self._process_images(samples[\"image\"])\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66233448",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir= # WRITE DOWN A DIRECTORY WHERE YOU'D LIKE THE DATA TO BE STORED \n",
    "train_data = load_dataset(\n",
    "    'mhsotoudeh/astroclip', \n",
    "    split='train',\n",
    "    cache_dir=cache_dir,\n",
    "    #streaming=True,\n",
    ")\n",
    "\n",
    "train_data = train_data.with_format('torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e58fd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(\n",
    "            train_data,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=AstroClipCollator(\n",
    "                center_crop=144,\n",
    "                bands=[\"g\", \"r\", \"z\"],\n",
    "                m=0.03,\n",
    "                Q=20,\n",
    "            ),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1c8a58",
   "metadata": {},
   "source": [
    "Let's now get a quick example batch from our data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e873d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c83f8d",
   "metadata": {},
   "source": [
    "Now let's focus on the image content and plot it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6441ea6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_batch['image'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1c42f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the convention for images in pytorch is Channels x Height x Width,\n",
    "# but for imshow is Height x Width x Channels.\n",
    "plt.imshow(example_batch['image'][0].permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641ebb24",
   "metadata": {},
   "source": [
    "You can play around with two velcity models, a CNN based one (UNet) and a transformer based one (Vision Transformer from torchvision). Here is an example of a UNet that can be used for this problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80167ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import UNet2DModel\n",
    "\n",
    "velocity_model = UNet2DModel(\n",
    "    sample_size=144,        \n",
    "    in_channels=3, \n",
    "    out_channels=3,\n",
    "    layers_per_block=2,\n",
    "    block_out_channels=(64, 64, 128, 256),  \n",
    "    down_block_types= tuple(['DownBlock2D'] * 4),\n",
    "    up_block_types= tuple(['UpBlock2D'] * 4),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5f1da7",
   "metadata": {},
   "source": [
    "Have fun :P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90888b72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
