{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b953a33",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Continuous Normalizing Flows (CNFs) are a class of generative models that learn to transform a simple base distribution (like a Gaussian) into a complex target distribution through continuous-time dynamics. Unlike discrete normalizing flows that use a finite sequence of invertible transformations, CNFs model the transformation as a continuous ordinary differential equation (ODE). CNFs are by definition invertible, regardless of the particular architecture used to model the velocity field.\n",
    "\n",
    "\n",
    "A CNF defines a continuous transformation from time $t=0$ to $t=1$:\n",
    "\n",
    "$$\\frac{dx_t}{dt} = v_\\theta(x_t, t)$$\n",
    "\n",
    "where:\n",
    "- $x_t \\in \\mathbb{R}^d$ is the state at time $t$\n",
    "- $v_\\theta(x_t, t)$ is a neural network (velocity field) parameterized by $\\theta$\n",
    "- The initial condition is $x_0 \\sim p_0$ (base distribution, typically Gaussian)\n",
    "\n",
    "For continuous transformations, the change of variables formula becomes:\n",
    "\n",
    "$$\\log p_1(x_1) = \\log p_0(x_0) - \\int_0^1 \\text{div}(v_\\theta(x_t, t)) \\, dt$$\n",
    "\n",
    "where $\\text{div}(v_\\theta(x_t, t))$ is the divergence of the velocity field.\n",
    "\n",
    "\n",
    "\n",
    "## Key Advantages of CNFs\n",
    "\n",
    "1. **Invertibility**: The transformation is naturally invertible through the ODE, regardless of the particular models' architecture\n",
    "2. **Exact Likelihood**: Can compute exact log-likelihoods \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f15d11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install torch torchvision torchaudio\n",
    "%pip install torchdiffeq\n",
    "%pip install matplotlib\n",
    "%pip install numpy\n",
    "%pip install tqdm\n",
    "%pip install ipython\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6e9f16",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/florpi/heidelberg_generative_lectures/blob/main/2_cnfs.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb480979",
   "metadata": {},
   "source": [
    "Let's try CNFs on the two moon problem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab1f2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591e7072",
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_moons(n: int, sigma: float = 5e-2):\n",
    "    theta = 2 * torch.pi * torch.rand(n)\n",
    "    label = (theta > torch.pi).float()\n",
    "\n",
    "    x = torch.stack(\n",
    "        (\n",
    "            torch.cos(theta) + label - 1 / 2,\n",
    "            torch.sin(theta) + label / 2 - 1 / 4,\n",
    "        ),\n",
    "        axis=-1,\n",
    "    )\n",
    "\n",
    "    return torch.normal(x, sigma), label\n",
    "\n",
    "\n",
    "samples, labels = two_moons(16384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bba11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(4.8, 4.8))\n",
    "plt.hist2d(*samples.T, bins=64, range=((-2, 2), (-2, 2)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43543259",
   "metadata": {},
   "source": [
    "\n",
    "## Velocity Network Implementation\n",
    "\n",
    "The velocity network is the core component of our CNF. It learns to predict the velocity field $v_\\theta(x_t, t)$ that defines how particles move through the state space.\n",
    "\n",
    "The network architecture follows the pattern:\n",
    "$$\\text{Input: } (x_t, y_t, t) \\rightarrow \\text{Hidden Layers} \\rightarrow \\text{Output: } (v_x, v_y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa92fe08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchdiffeq import odeint\n",
    "from torch import nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ac94b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VelocityNetwork(nn.Module):\n",
    "    def __init__(self, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        # TODO: Create a neural network that takes as input:\n",
    "        # - 2D position (x_t): 2 dimensions  \n",
    "        # - 1D time (t): 1 dimension\n",
    "        # Total input: 3 dimensions\n",
    "        # \n",
    "        # Output: 2D velocity vector\n",
    "        #\n",
    "        # Store this in self.net as a nn.Sequential\n",
    "        \n",
    "        #self.net = # Your code here\n",
    "\n",
    "\n",
    "    def forward(self, xt, t):\n",
    "        # TODO: Combine xt and t into a single input tensor and pass it through the \n",
    "        # velocity prediction network\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d546ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "x1, _  = two_moons(500)\n",
    "x1 = x1.to(device).float()  # Move to device and convert to float\n",
    "\n",
    "velocity_nn = VelocityNetwork(\n",
    "    hidden_dim=256,\n",
    ").to(device)\n",
    "t = torch.full((500,), 0.5, dtype=torch.float32, device=device)\n",
    "\n",
    "assert velocity_nn(\n",
    "    xt=x1,\n",
    "    t=t,\n",
    ").shape == (500, 2), \"Expected shape (500, 2) but got different shape\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61aa6232",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Divergence Computation\n",
    "\n",
    "The divergence of the velocity field is crucial for computing the change in probability density. For a 2D velocity field $v = (v_x, v_y)$, the divergence is:\n",
    "\n",
    "$$\\text{div}(v) = \\frac{\\partial v_x}{\\partial x} + \\frac{\\partial v_y}{\\partial y}$$\n",
    "\n",
    "**Why is divergence important?**\n",
    "- It measures how much the velocity field \"spreads out\" or \"converges\" at each point\n",
    "- Positive divergence: flow is expanding (probability density decreases)\n",
    "- Negative divergence: flow is contracting (probability density increases)\n",
    "- The integral of divergence over time gives the total change in log-probability\n",
    "\n",
    "**Implementation details:**\n",
    "- We use PyTorch's automatic differentiation to compute partial derivatives\n",
    "- The `retain_graph=True` option allows us to compute gradients of gradients\n",
    "- We sum over all spatial dimensions to get the total divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dde58db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_div(y, x):\n",
    "    # TODO: Implement the divergence of a vector field y, respect to x\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77eb308a",
   "metadata": {},
   "source": [
    "Let's test your implementation with an analytical solution, for the function $v(x, y) = (x^3, y^3)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36b332d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = torch.randn((10, 2), requires_grad=True)\n",
    "test_outputs = test_inputs**3\n",
    "        \n",
    "div_result = compute_div(test_outputs, test_inputs).squeeze()\n",
    "expected_div = (3 * test_inputs**2).sum(dim=-1)\n",
    "assert torch.allclose(div_result, expected_div, atol=1e-6, rtol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41607e5f",
   "metadata": {},
   "source": [
    "Now it is important that we can compute the gradients of the loss function, which can be tricky in pytorch since it requires second order differentiation. so you need to make sure the computational graph is kept so that we can differentiate it again (hint, checkout create_graph in torch.autograd.grad). This will use more memory, but it's crucial to be able to differentiate again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c6c2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.enable_grad():\n",
    "    test_inputs = torch.randn((10, 2), requires_grad=True)\n",
    "    test_outputs = test_inputs**3\n",
    "    div_result = compute_div(test_outputs, test_inputs).squeeze()\n",
    "\n",
    "    # Now let's compute a dummy loss and differentiate it again\n",
    "    loss = (div_result**2).mean()\n",
    "    loss.backward()\n",
    "    actual_grad_x = test_inputs.grad[:, 0]\n",
    "\n",
    "\n",
    "    N = test_inputs.size(0)\n",
    "    expected_grad_x = (12 / N) * div_result.detach() * test_inputs[:, 0]\n",
    "            \n",
    "    assert torch.allclose(actual_grad_x, expected_grad_x, atol=1e-4, rtol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2e8f31",
   "metadata": {},
   "source": [
    "## Training Process\n",
    "\n",
    "The training loop implements maximum likelihood estimation for the CNF:\n",
    "\n",
    "### Loss Function\n",
    "We minimize the negative log-likelihood:\n",
    "$$\\mathcal{L}(\\theta) = -\\mathbb{E}_{x \\sim p_{\\text{data}}}[\\log p_1(x)]$$\n",
    "\n",
    "To compute $\\log p_1(x_1)$, we solve a joint ODE that tracks both the state and the accumulated divergence:\n",
    "\n",
    "$$\\frac{d}{dt}\\begin{bmatrix} x_t \\\\ \\log p_t(x_t) \\end{bmatrix} = \\begin{bmatrix} v_\\theta(x_t, t) \\\\ -\\text{div}(v_\\theta(x_t, t)) \\end{bmatrix}$$\n",
    "\n",
    "\n",
    "The explicit loss function is:\n",
    "\n",
    "$$\\mathcal{L}(\\theta) = -\\mathbb{E}_{x \\sim p_{\\text{data}}}[\\log p_0(x_0) - \\int_0^1 \\text{div}(v_\\theta(x_t, t)) \\, dt]$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d0dbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_log_prior(x0):\n",
    "    # TODO: write down the log prior for x0, our base distribution\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f16f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "\n",
    "class ContinuosTimeFlow(nn.Module):\n",
    "    def __init__(self, velocity_network, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.velocity_net = velocity_network\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "    def sample(self, target_shape=(2,), num_samples=10,):\n",
    "        #TODO: Solve the sampling ode starting from x0, and return the final state x1 (hint odeint from torchdiffeq could be your friend)\n",
    "        return x_1\n",
    "\n",
    "\n",
    "    def rhs_of_joint_ode(self, t, x_t_div_t,):\n",
    "        # TODO: Implement the right hand side of the joint ode needed for sampling p(x)\n",
    "        return\n",
    "\n",
    "            \n",
    "    def log_prob(self, x):\n",
    "        # TODO: Implement the log probability of the flow\n",
    "        return\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654b79af",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow = ContinuosTimeFlow(velocity_nn, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398ec6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "logp = flow.log_prob(x1)\n",
    "\n",
    "# let's make sure you are returning a scalar value for each example in the batch\n",
    "assert logp.shape == (x1.shape[0],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f95711",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from IPython.display import clear_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46583860",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(flow.parameters(), lr=1.e-4, weight_decay=1e-4)\n",
    "total_steps = 1_000\n",
    "generate_every = 100\n",
    "batch_size = 128\n",
    "losses = []\n",
    "\n",
    "flow.train()\n",
    "\n",
    "step = 0 \n",
    "with tqdm(total=total_steps, desc=\"Training\") as pbar:\n",
    "    while step < total_steps:\n",
    "        x, label = two_moons(batch_size)\n",
    "        x = x.to(device)  # Move data to device\n",
    "        loss = -flow.log_prob(x).mean()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(flow.parameters(), max_norm=1.0)\n",
    "\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "        pbar.set_postfix({\n",
    "                'Loss': f'{loss:.4f}',\n",
    "            })\n",
    "        pbar.update(1)\n",
    "        step += 1\n",
    "        losses.append(loss.detach())\n",
    "        if step % generate_every == 0:\n",
    "            flow.velocity_net.eval()\n",
    "            print('x = ', x.shape)\n",
    "            print(f\"\\nGenerating samples at step {step}...\")\n",
    "            \n",
    "            samples = flow.sample(\n",
    "                num_samples=500, \n",
    "            ).detach().cpu().numpy()\n",
    "            clear_output(wait=True)\n",
    "            plt.scatter(\n",
    "                x[:,0].cpu().numpy(),\n",
    "                x[:,1].cpu().numpy(),\n",
    "                c='indianred',\n",
    "                s=1,\n",
    "                alpha=0.8,\n",
    "                label='Original'\n",
    "            )\n",
    "            plt.scatter(\n",
    "                samples[:,0],\n",
    "                samples[:,1],\n",
    "                c='k',\n",
    "                s=1,\n",
    "                label='Generated'\n",
    "            )\n",
    "            plt.legend()\n",
    "            plt.title(f'Generated Samples at step {step}')\n",
    "            plt.axis('off')\n",
    " \n",
    "            plt.show()\n",
    "            flow.velocity_net.train()\n",
    "\n",
    "    losses = torch.stack(losses)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac4cb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd22775",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
